---
layout: page
title: GIS and Uncertainty
---
When doing any kind of GIS work, uncertainty is introduced from the beginning. Data used in GIS is never fully accurate or complete, as we cannot summarize the world accurately or the reality on the ground with just data (Longley et al. 2005). Geographic units are never objective, or truly based in some kind of objective reality, as they all require transforming point events into area objects (Langley et al. 2005). There will always be some level of abstraction, but introducing this kind of uncertainty is the only way to conduct analyses and form theories about how the world and societies operate.

Uncertainty is introduced through a specific process of analysis:
real world --> conception --> measurement and representation --> analysis, where the arrows all represent opportunities for distortion from the true reality (Langley et al. 2005). Arguably the most important part of published research, the analysis, and the conclusions drawn from the analysis, are the furthest point away from reality. Analyses will always have a level of ambiguity and vagueness, as indicators of phenomenon, whether they be direct, like household income, or indirect, like rates of car ownership, do not represent reality themselves.

Geographers have a responsibility to be honest about the uncertainty in their research. Doing so means being up front about assumptions made, and potential weaknesses starting with the data itself all the way to the final analysis. Longley (2005) suggests using subjective probability to create fuzzy boundaries to classes, allowing object's degree of belonging to a class to be partial in order to capture the uncertainty inherent in the assignment of places and data in the real world to specific categories.

My thesis, in some ways, centers around uncertainty in spatial research. I'm using declassified DOE maps from nuclear testing in Nevada and the Marshall Islands to uncover inequitable exposure of indigenous peoples to radioactive fallout. Mapping fallout is inherently difficult and not particularly accurate, especially when nuclear testing was first being conducted. It is impossible to measure the amount of radioactivity in every single particle of air, so no matter how accurate these maps could be, they would never be a truly accurate representations of conditions on the ground. Moreover, the models used to predict how and where a radioactive plume would travel have changed over time, meaning these declassified maps likely use models that would not be considered particularly accurate by today's standards. However, these maps are what the US government produced, and use for their own internal purposes, so if they prove inequitable exposure to fallout, then they can be used within the government system, particularly in the judicial system, to gain rightful compensation. 


Sources:
* Longley, P. A., M. F. Goodchild, D. J. Maguire, and D. W. Rhind. 2008. Geographical information systems and science 2nd ed. Chichester: Wiley. (only chapter 6: Uncertainty, pages 127-153)
